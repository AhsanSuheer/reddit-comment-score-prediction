{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.svm import SVC\n",
    "import string\n",
    "import random\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, confusion_matrix, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from wordcloud import WordCloud\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 10000\n",
    "\n",
    "# Load in the two datasets\n",
    "comments_neg = pd.read_csv(\"data/comments_negative.csv\")\n",
    "comments_pos = pd.read_csv(\"data/comments_positive.csv\")\n",
    "\n",
    "# Sample 20,000 tweets\n",
    "comments_neg = comments_neg.sample(size)\n",
    "comments_pos = comments_pos.sample(size)\n",
    "\n",
    "# Remove invalid entries\n",
    "comments_neg = comments_neg[~pd.isna(comments_neg['text'])]\n",
    "comments_pos = comments_pos[~pd.isna(comments_pos['text'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize mean and standard deviation of scores of comments\n",
    "pos_stats = [comments_pos['score'].mean(), comments_pos['score'].std()]\n",
    "neg_stats = [abs(comments_neg['score'].mean()), abs(comments_neg['score'].std())]\n",
    "\n",
    "bar_pos_1 = np.arange(len(pos_stats))\n",
    "bar_pos_2 = bar_pos_1 + 0.25\n",
    "\n",
    "plt.bar(bar_pos_1, pos_stats, color = 'b', width = 0.25, label = 'Positive')\n",
    "plt.bar(bar_pos_2, neg_stats, color = 'r', width = 0.25, label = 'Negative')\n",
    "plt.xticks([i + 0.125 for i in range(len(pos_stats))], ['mean', 'standard deviation'])\n",
    "plt.ylabel(\"Value\")\n",
    "plt.yticks(list(range(0, 250, 20)))\n",
    "plt.legend()\n",
    "plt.xlabel(\"Statistic\")\n",
    "plt.title(\"Reddit Comment Scores’ Analysis\")\n",
    "# plt.savefig(\"stats\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize boxplots of comment scores\n",
    "boxplot_data = [comments_pos['score'], comments_neg['score'].abs()]\n",
    "\n",
    "plt.figure(figsize = (8,5))\n",
    "plt.boxplot(boxplot_data, patch_artist = True, showfliers = False, labels = [\"Positive\", \"Negative\"])\n",
    "plt.yticks(list(range(0, 400, 20)))\n",
    "plt.title(\"Reddit Comment Scores' Analysis\")\n",
    "# plt.savefig(\"boxplot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store stopwords and punctuation to be removed\n",
    "stopwords_eng = set(stopwords.words('english'))\n",
    "punctuation = list(string.punctuation)\n",
    "\n",
    "# Function that returns list of unigrams from sentence after pre-processing it\n",
    "def tokenizer(sentences):\n",
    "    unigrams = []\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        unigrams.extend([word for word in words if word not in stopwords_eng and word not in punctuation])\n",
    "\n",
    "    return unigrams\n",
    "\n",
    "# Generate list of all unigrams\n",
    "unigrams = tokenizer(comments_pos['text'].tolist())\n",
    "unigrams.extend(tokenizer(comments_neg['text'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the vocabulary using a threshold of 5\n",
    "def build_vocab(unigrams):\n",
    "    unigrams = [word.lower() for word in unigrams]\n",
    "    unigram_counts = dict(Counter(unigrams))\n",
    "    vocab = []\n",
    "    for unigram, unigram_count in unigram_counts.items():\n",
    "        if unigram_count > 5:\n",
    "            vocab.append(unigram)\n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocab(unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize texts using Wordclouds\n",
    "positive_text_str = \" \".join(tokenizer(comments_pos['text'].tolist()))\n",
    "negative_text_str = \" \".join(tokenizer(comments_neg['text'].tolist()))\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title(\"Positive Comments\")\n",
    "plt.imshow(WordCloud(max_font_size=40).generate(positive_text_str))\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title(\"Negative Comments\")\n",
    "plt.imshow(WordCloud(max_font_size=40).generate(negative_text_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_analyzer = SentimentIntensityAnalyzer()\n",
    "sent_scores = [[],[],[]]\n",
    "\n",
    "negative_comments_list = comments_neg['text'].tolist()\n",
    "\n",
    "# Store sentiment scores for plots\n",
    "for neg_comment in negative_comments_list:\n",
    "    sent_analysis = sent_analyzer.polarity_scores(neg_comment)\n",
    "    sent_scores[0].append(sent_analysis[\"neg\"])\n",
    "    sent_scores[1].append(sent_analysis[\"neu\"])\n",
    "    sent_scores[2].append(sent_analysis[\"pos\"])\n",
    "\n",
    "# Store sentiment score means for plots\n",
    "means = [sum(x)/len(x) for x in sent_scores]\n",
    "\n",
    "\n",
    "# Plot the negative comments data\n",
    "fig, axs = plt.subplots(1,3, figsize = (16,5))\n",
    "\n",
    "axs[0].hist(sent_scores[0])\n",
    "axs[0].set_title(f\"Negativity \\n(μ = {round(means[0], 3)})\")\n",
    "axs[1].hist(sent_scores[1])\n",
    "axs[1].set_title(f\"Neutrality \\n(μ = {round(means[1], 3)})\")\n",
    "axs[2].hist(sent_scores[2])\n",
    "axs[2].set_title(f\"Positivity \\n(μ = {round(means[2], 3)})\")\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_xlabel(\"Sentiment Score\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.set_xticks((0,0.5,1))\n",
    "    ax.label_outer()\n",
    "\n",
    "fig.suptitle(\"Negative Comments\", y=1.05)\n",
    "# fig.savefig(\"neg_sent\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the positive comments data\n",
    "sent_analyzer = SentimentIntensityAnalyzer()\n",
    "sent_scores = [[],[],[]]\n",
    "\n",
    "positive_comments_list = comments_pos['text'].tolist()\n",
    "\n",
    "for pos_comment in positive_comments_list:\n",
    "    sent_analysis = sent_analyzer.polarity_scores(pos_comment)\n",
    "    sent_scores[0].append(sent_analysis[\"neg\"])\n",
    "    sent_scores[1].append(sent_analysis[\"neu\"])\n",
    "    sent_scores[2].append(sent_analysis[\"pos\"])\n",
    "\n",
    "means = [sum(x)/len(x) for x in sent_scores]\n",
    "\n",
    "fig, axs = plt.subplots(1,3, figsize = (16,5))\n",
    "\n",
    "axs[0].hist(sent_scores[0])\n",
    "axs[0].set_title(f\"Negativity \\n(μ = {round(means[0], 3)})\")\n",
    "axs[1].hist(sent_scores[1])\n",
    "axs[1].set_title(f\"Neutrality \\n(μ = {round(means[1], 3)})\")\n",
    "axs[2].hist(sent_scores[2])\n",
    "axs[2].set_title(f\"Positivity \\n(μ = {round(means[2], 3)})\")\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_xlabel(\"Sentiment Score\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.set_xticks((0,0.5,1))\n",
    "    ax.label_outer()\n",
    "\n",
    "fig.suptitle(\"Positive Comments\", y=1.05)\n",
    "# fig.savefig(\"pos_sent\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that generates bag of words representation for sentences\n",
    "def generate_bow(sentences):\n",
    "    k = 0\n",
    "    feature_matrix = []\n",
    "    for sentence in sentences:\n",
    "        if (k % 10000 == 0):\n",
    "            print(k)\n",
    "        sentence_words = word_tokenize(sentence)\n",
    "        sentence_words = [word for word in sentence_words if word not in stopwords_eng and word not in punctuation]\n",
    "        bow_vector = [0] * len(vocab)\n",
    "        for word in sentence_words:\n",
    "            for i,vocab_word in enumerate(vocab):\n",
    "                if word == vocab_word:\n",
    "                    bow_vector[i] += 1\n",
    "        feature_matrix.append(bow_vector)\n",
    "        k += 1\n",
    "    return feature_matrix\n",
    "\n",
    "all_comments = comments_pos['text'].tolist()\n",
    "all_comments.extend(comments_neg['text'].tolist())\n",
    "\n",
    "feature_matrix = generate_bow(all_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate labels (1 if positive, -1 if negative)\n",
    "labels = np.ones(len(feature_matrix))\n",
    "comments_df = pd.concat([comments_pos, comments_neg])\n",
    "\n",
    "# Add meta features\n",
    "feature_names = vocab.copy()\n",
    "meta_features = [\"controversiality\", \"pos_sent\", \"neg_sent\", \"parent_pos_sent\", \n",
    "                 \"parent_neg_sent\", \"parent_score\", \"parent_controversiality\"]\n",
    "feature_names.extend(meta_features)\n",
    "\n",
    "sent_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Add meta features to feature matrix\n",
    "for i,row_tuple in enumerate(comments_df.iterrows()):\n",
    "    row = row_tuple[1]\n",
    "    if row['score'] < 0:\n",
    "        labels[i] = -1\n",
    "    feature_matrix[i].append(int(row['controversiality']))\n",
    "    comment_sent_scores = sent_analyzer.polarity_scores(row['text'])\n",
    "    feature_matrix[i].append(comment_sent_scores[\"pos\"])\n",
    "    feature_matrix[i].append(comment_sent_scores[\"neg\"])\n",
    "    parent_sent_scores = sent_analyzer.polarity_scores(row['parent_text'])\n",
    "    feature_matrix[i].append(parent_sent_scores[\"pos\"])\n",
    "    feature_matrix[i].append(parent_sent_scores[\"neg\"])\n",
    "    feature_matrix[i].append(int(row['parent_score']))\n",
    "    feature_matrix[i].append(int(row['parent_controversiality']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate train and test sets\n",
    "\n",
    "labels = [int(label) for label in labels]\n",
    "zipped = list(zip(feature_matrix, labels))\n",
    "random.shuffle(zipped)\n",
    "split_point = int(0.8*len(zipped))\n",
    "train_zipped = zipped[:split_point]\n",
    "test_zipped = zipped[split_point:]\n",
    "x_train, y_train = zip(*train_zipped)\n",
    "x_test, y_test = zip(*test_zipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# majority baseline\n",
    "print(\"Majority Baseline\")\n",
    "y_pred = [1] * len(y_test)\n",
    "print(\"f1 score:\", f1_score(y_test, y_pred))\n",
    "print(\"precision score:\", precision_score(y_test, y_pred))\n",
    "print(\"recall score:\", recall_score(y_test, y_pred))\n",
    "print(\"accuracy score:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# random baseline\n",
    "print(\"Random Baseline\")\n",
    "y_pred = np.random.randint(0,2,len(y_test))\n",
    "y_pred = [-1 if x == 0 else 1 for x in y_pred ]\n",
    "print(\"f1 score:\", f1_score(y_test, y_pred))\n",
    "print(\"precision score:\", precision_score(y_test, y_pred))\n",
    "print(\"recall score:\", recall_score(y_test, y_pred))\n",
    "print(\"accuracy score:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit an SVM classifier with linear kernel\n",
    "svm_classifier = SVC(C = 1, kernel = \"linear\", max_iter = 7000, verbose = True)\n",
    "svm_classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test set scores for SVM classifier\n",
    "y_pred = svm_classifier.predict(x_test)\n",
    "print(\"f1 score:\", f1_score(y_test, y_pred))\n",
    "print(\"precision score:\", precision_score(y_test, y_pred))\n",
    "print(\"recall score:\", recall_score(y_test, y_pred))\n",
    "print(\"accuracy score:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "svm_coef = svm_classifier.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top features for SVM\n",
    "svm_coef = svm_coef.ravel()\n",
    "\n",
    "top_positive_coefficients = np.argsort(svm_coef)[-20:]\n",
    "top_negative_coefficients = np.argsort(svm_coef)[:20]\n",
    "feature_names = np.asarray(feature_names)\n",
    "\n",
    "top_pos_features = feature_names[top_positive_coefficients][::-1]\n",
    "top_pos_weights = svm_coef[top_positive_coefficients][::-1]\n",
    "top_neg_features = feature_names[top_negative_coefficients]\n",
    "top_neg_weights = svm_coef[top_negative_coefficients]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top positive features for SVM\n",
    "col = ['blue'] * len(top_neg_weights)\n",
    "imp = [abs(x) for x in top_pos_weights]\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.barh(range(len(top_pos_features)), imp, color = col, align='center')\n",
    "plt.yticks(range(len(top_pos_features)), top_pos_features)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.ylabel(\"Feature Name\")\n",
    "plt.xlabel(\"Feature Weight\")\n",
    "plt.title(\"Feature Importance for Positive Comments\")\n",
    "# plt.savefig(\"pos_feature_importance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top negative features for SVM\n",
    "col = ['red'] * len(top_neg_weights)\n",
    "imp = [x for x in top_neg_weights]\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.barh(range(len(top_neg_features)), imp, color = col, align='center')\n",
    "plt.yticks(range(len(top_neg_features)), top_neg_features)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.ylabel(\"Feature Name\")\n",
    "plt.xlabel(\"Feature Weight\")\n",
    "plt.title(\"Feature Importance for Negative Comments\")\n",
    "# plt.savefig(\"neg_feature_importance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and evaluate a Logistic Regression model\n",
    "lr_model = LogisticRegression(max_iter=2000, verbose=1, solver=\"liblinear\")\n",
    "lr_model.fit(x_train, y_train)\n",
    "\n",
    "y_pred = lr_model.predict(x_test)\n",
    "print(\"f1 score:\", f1_score(y_test, y_pred))\n",
    "print(\"precision score:\", precision_score(y_test, y_pred))\n",
    "print(\"recall score:\", recall_score(y_test, y_pred))\n",
    "print(\"accuracy score:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and evaluate a Random Forest model\n",
    "rf_classifier = RandomForestClassifier(300)\n",
    "rf_classifier.fit(x_train, y_train)\n",
    "\n",
    "y_pred = rf_classifier.predict(x_test)\n",
    "print(\"f1 score:\", f1_score(y_test, y_pred))\n",
    "print(\"precision score:\", precision_score(y_test, y_pred))\n",
    "print(\"recall score:\", recall_score(y_test, y_pred))\n",
    "print(\"accuracy score:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate top features for Random Forest model\n",
    "rf_permutation_importance = permutation_importance(rf_classifier, x_test, y_test)\n",
    "\n",
    "sorted_indices = rf_permutation_importance.importances_mean.argsort()\n",
    "plt.barh(feature_names[sorted_indices], perm_importance.importances_mean[sorted_indices])\n",
    "plt.xlabel(\"Permutation Importance\")\n",
    "plt.ylabel(\"Feature Name\")\n",
    "plt.title(\"Feature Importance for Random Forest\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
